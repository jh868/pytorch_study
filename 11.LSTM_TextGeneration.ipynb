{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['abstract', 'articleID', 'articleWordCount', 'byline', 'documentType',\n",
      "       'headline', 'keywords', 'multimedia', 'newDesk', 'printPage', 'pubDate',\n",
      "       'sectionName', 'snippet', 'source', 'typeOfMaterial', 'webURL'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import os\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./NewYorkTimes/ArticlesApril2017.csv')\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class TextGeneration(Dataset):\n",
    "    def clean_text(self, txt):\n",
    "        txt = ''.join(v for v in txt if v not in string.punctuation).lower()\n",
    "        return txt\n",
    "\n",
    "    def __init__(self):\n",
    "        all_headlines = []\n",
    "\n",
    "        for filename in glob.glob('./NewYorkTimes/*.csv'):\n",
    "            if 'Articles' in filename:\n",
    "                article_df = pd.read_csv(filename)\n",
    "\n",
    "                all_headlines.extend(list(article_df.headline.values))\n",
    "                break\n",
    "\n",
    "        all_headlines = [h for h in all_headlines if h!= 'Unknown']\n",
    "\n",
    "        self.corpus = [self.clean_text(x) for x in all_headlines]\n",
    "        self.BOW = {}\n",
    "\n",
    "        for line in self.corpus:\n",
    "            for word in line.split():\n",
    "                if word not in self.BOW.keys():\n",
    "                    self.BOW[word] = len(self.BOW.keys())\n",
    "\n",
    "        self.data = self.generate_sequence(self.corpus)\n",
    "\n",
    "    def generate_sequence(self, txt):\n",
    "        seq =[]\n",
    "\n",
    "        for line in txt:\n",
    "            line = line.split()\n",
    "            line_bow = [self.BOW[word] for word in line]\n",
    "\n",
    "            data = [([line_bow[i], line_bow[i+1]], line_bow[i+2]) for i in range(len(line_bow)-2)]\n",
    "\n",
    "            seq.extend(data)\n",
    "\n",
    "        return seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        data = np.array(self.data[i][0])\n",
    "        label = np.array(self.data[i][1]).astype(np.float32)\n",
    "\n",
    "        return data, label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_embeddings):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=16)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=16,\n",
    "            hidden_size=64,\n",
    "            num_layers=5,\n",
    "            batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(128, num_embeddings)\n",
    "        self.fc2 = nn.Linear(num_embeddings, num_embeddings)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "\n",
    "        x, _ = self.lstm(x)\n",
    "        x = torch.reshape(x, (x.shape[0], -1))\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]C:\\Users\\labadmin\\AppData\\Local\\Temp\\2\\ipykernel_6636\\2814728616.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred = model(torch.tensor(data, dtype=torch.long).to(device))\n",
      "C:\\Users\\labadmin\\AppData\\Local\\Temp\\2\\ipykernel_6636\\2814728616.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = nn.CrossEntropyLoss()(pred, torch.tensor(label, dtype=torch.long).to(device))\n",
      "epoch: 1 loss: 7.36794900894165: 100%|██████████| 63/63 [00:03<00:00, 17.23it/s]  \n",
      "epoch: 2 loss: 7.008695125579834: 100%|██████████| 63/63 [00:00<00:00, 99.85it/s]  \n",
      "epoch: 3 loss: 6.715979099273682: 100%|██████████| 63/63 [00:00<00:00, 94.03it/s]  \n",
      "epoch: 4 loss: 6.485108375549316: 100%|██████████| 63/63 [00:00<00:00, 97.98it/s] \n",
      "epoch: 5 loss: 6.385385036468506: 100%|██████████| 63/63 [00:00<00:00, 101.45it/s] \n",
      "epoch: 6 loss: 6.29008674621582: 100%|██████████| 63/63 [00:00<00:00, 95.31it/s]  \n",
      "epoch: 7 loss: 6.172379493713379: 100%|██████████| 63/63 [00:00<00:00, 97.53it/s] \n",
      "epoch: 8 loss: 5.9537858963012695: 100%|██████████| 63/63 [00:00<00:00, 96.78it/s] \n",
      "epoch: 9 loss: 5.851760387420654: 100%|██████████| 63/63 [00:00<00:00, 93.20it/s] \n",
      "epoch: 10 loss: 5.621973037719727: 100%|██████████| 63/63 [00:00<00:00, 95.89it/s] \n",
      "epoch: 11 loss: 5.454799175262451: 100%|██████████| 63/63 [00:00<00:00, 95.89it/s] \n",
      "epoch: 12 loss: 5.426936626434326: 100%|██████████| 63/63 [00:00<00:00, 94.03it/s] \n",
      "epoch: 13 loss: 5.333698272705078: 100%|██████████| 63/63 [00:00<00:00, 98.90it/s]  \n",
      "epoch: 14 loss: 5.269402503967285: 100%|██████████| 63/63 [00:00<00:00, 101.30it/s] \n",
      "epoch: 15 loss: 5.301072120666504: 100%|██████████| 63/63 [00:00<00:00, 95.89it/s]  \n",
      "epoch: 16 loss: 5.331037998199463: 100%|██████████| 63/63 [00:00<00:00, 100.81it/s]\n",
      "epoch: 17 loss: 5.367950439453125: 100%|██████████| 63/63 [00:00<00:00, 97.53it/s]  \n",
      "epoch: 18 loss: 5.243948936462402: 100%|██████████| 63/63 [00:00<00:00, 94.17it/s] \n",
      "epoch: 19 loss: 5.086899757385254: 100%|██████████| 63/63 [00:00<00:00, 99.37it/s]  \n",
      "epoch: 20 loss: 5.232287883758545: 100%|██████████| 63/63 [00:00<00:00, 98.60it/s]  \n",
      "epoch: 21 loss: 5.310571193695068: 100%|██████████| 63/63 [00:00<00:00, 93.89it/s] \n",
      "epoch: 22 loss: 5.202145576477051: 100%|██████████| 63/63 [00:00<00:00, 98.75it/s] \n",
      "epoch: 23 loss: 4.978562355041504: 100%|██████████| 63/63 [00:00<00:00, 95.75it/s] \n",
      "epoch: 24 loss: 4.571106433868408: 100%|██████████| 63/63 [00:00<00:00, 92.38it/s] \n",
      "epoch: 25 loss: 4.320580005645752: 100%|██████████| 63/63 [00:00<00:00, 97.68it/s] \n",
      "epoch: 26 loss: 4.09630012512207: 100%|██████████| 63/63 [00:00<00:00, 100.48it/s]  \n",
      "epoch: 27 loss: 3.931320905685425: 100%|██████████| 63/63 [00:00<00:00, 92.11it/s] \n",
      "epoch: 28 loss: 3.9033102989196777: 100%|██████████| 63/63 [00:00<00:00, 100.32it/s]\n",
      "epoch: 29 loss: 3.7808544635772705: 100%|██████████| 63/63 [00:00<00:00, 100.64it/s]\n",
      "epoch: 30 loss: 3.7117362022399902: 100%|██████████| 63/63 [00:00<00:00, 95.17it/s]\n",
      "epoch: 31 loss: 3.7055671215057373: 100%|██████████| 63/63 [00:00<00:00, 100.64it/s]\n",
      "epoch: 32 loss: 3.702772855758667: 100%|██████████| 63/63 [00:00<00:00, 100.00it/s] \n",
      "epoch: 33 loss: 3.707883358001709: 100%|██████████| 63/63 [00:00<00:00, 94.74it/s] \n",
      "epoch: 34 loss: 3.754082441329956: 100%|██████████| 63/63 [00:00<00:00, 101.13it/s] \n",
      "epoch: 35 loss: 3.6353747844696045: 100%|██████████| 63/63 [00:00<00:00, 98.90it/s] \n",
      "epoch: 36 loss: 3.485778331756592: 100%|██████████| 63/63 [00:00<00:00, 94.74it/s] \n",
      "epoch: 37 loss: 3.392216444015503: 100%|██████████| 63/63 [00:00<00:00, 98.59it/s] \n",
      "epoch: 38 loss: 3.3953213691711426: 100%|██████████| 63/63 [00:00<00:00, 102.94it/s]\n",
      "epoch: 39 loss: 3.1894710063934326: 100%|██████████| 63/63 [00:00<00:00, 93.47it/s]\n",
      "epoch: 40 loss: 3.3837714195251465: 100%|██████████| 63/63 [00:00<00:00, 100.00it/s]\n",
      "epoch: 41 loss: 3.5189743041992188: 100%|██████████| 63/63 [00:00<00:00, 98.75it/s] \n",
      "epoch: 42 loss: 3.728175401687622: 100%|██████████| 63/63 [00:00<00:00, 95.90it/s] \n",
      "epoch: 43 loss: 3.487441301345825: 100%|██████████| 63/63 [00:00<00:00, 96.93it/s] \n",
      "epoch: 44 loss: 3.305196762084961: 100%|██████████| 63/63 [00:00<00:00, 100.00it/s] \n",
      "epoch: 45 loss: 3.45951771736145: 100%|██████████| 63/63 [00:00<00:00, 96.19it/s]  \n",
      "epoch: 46 loss: 3.4365010261535645: 100%|██████████| 63/63 [00:00<00:00, 100.00it/s]\n",
      "epoch: 47 loss: 3.3191871643066406: 100%|██████████| 63/63 [00:00<00:00, 98.91it/s] \n",
      "epoch: 48 loss: 3.1493382453918457: 100%|██████████| 63/63 [00:00<00:00, 95.03it/s]\n",
      "epoch: 49 loss: 3.0041165351867676: 100%|██████████| 63/63 [00:00<00:00, 98.75it/s] \n",
      "epoch: 50 loss: 2.981828212738037: 100%|██████████| 63/63 [00:00<00:00, 101.78it/s] \n",
      "epoch: 51 loss: 2.9308316707611084: 100%|██████████| 63/63 [00:00<00:00, 95.73it/s]\n",
      "epoch: 52 loss: 2.9223523139953613: 100%|██████████| 63/63 [00:00<00:00, 100.80it/s]\n",
      "epoch: 53 loss: 2.844407081604004: 100%|██████████| 63/63 [00:00<00:00, 100.32it/s] \n",
      "epoch: 54 loss: 2.8142247200012207: 100%|██████████| 63/63 [00:00<00:00, 96.48it/s]\n",
      "epoch: 55 loss: 2.7498066425323486: 100%|██████████| 63/63 [00:00<00:00, 100.00it/s]\n",
      "epoch: 56 loss: 2.8567581176757812: 100%|██████████| 63/63 [00:00<00:00, 100.64it/s]\n",
      "epoch: 57 loss: 2.8441226482391357: 100%|██████████| 63/63 [00:00<00:00, 95.75it/s]\n",
      "epoch: 58 loss: 2.768860340118408: 100%|██████████| 63/63 [00:00<00:00, 101.29it/s] \n",
      "epoch: 59 loss: 2.7655181884765625: 100%|██████████| 63/63 [00:00<00:00, 97.08it/s] \n",
      "epoch: 60 loss: 2.7337069511413574: 100%|██████████| 63/63 [00:00<00:00, 96.78it/s]\n",
      "epoch: 61 loss: 3.3078198432922363: 100%|██████████| 63/63 [00:00<00:00, 100.48it/s]\n",
      "epoch: 62 loss: 2.779331922531128: 100%|██████████| 63/63 [00:00<00:00, 102.28it/s] \n",
      "epoch: 63 loss: 2.7463316917419434: 100%|██████████| 63/63 [00:00<00:00, 95.89it/s]\n",
      "epoch: 64 loss: 2.9295012950897217: 100%|██████████| 63/63 [00:00<00:00, 100.32it/s]\n",
      "epoch: 65 loss: 2.8894076347351074: 100%|██████████| 63/63 [00:00<00:00, 99.37it/s] \n",
      "epoch: 66 loss: 2.813340425491333: 100%|██████████| 63/63 [00:00<00:00, 91.71it/s]  \n",
      "epoch: 67 loss: 2.633068084716797: 100%|██████████| 63/63 [00:00<00:00, 100.32it/s] \n",
      "epoch: 68 loss: 2.5423998832702637: 100%|██████████| 63/63 [00:00<00:00, 100.48it/s]\n",
      "epoch: 69 loss: 2.4915964603424072: 100%|██████████| 63/63 [00:00<00:00, 95.46it/s]\n",
      "epoch: 70 loss: 2.401441812515259: 100%|██████████| 63/63 [00:00<00:00, 98.91it/s]  \n",
      "epoch: 71 loss: 2.3813652992248535: 100%|██████████| 63/63 [00:00<00:00, 101.95it/s]\n",
      "epoch: 72 loss: 2.415440320968628: 100%|██████████| 63/63 [00:00<00:00, 96.63it/s]  \n",
      "epoch: 73 loss: 2.318971633911133: 100%|██████████| 63/63 [00:00<00:00, 99.53it/s]  \n",
      "epoch: 74 loss: 2.232011318206787: 100%|██████████| 63/63 [00:00<00:00, 100.64it/s] \n",
      "epoch: 75 loss: 2.2926125526428223: 100%|██████████| 63/63 [00:00<00:00, 96.78it/s] \n",
      "epoch: 76 loss: 2.3993983268737793: 100%|██████████| 63/63 [00:00<00:00, 100.32it/s]\n",
      "epoch: 77 loss: 2.361891984939575: 100%|██████████| 63/63 [00:00<00:00, 100.00it/s] \n",
      "epoch: 78 loss: 2.242129325866699: 100%|██████████| 63/63 [00:00<00:00, 95.17it/s] \n",
      "epoch: 79 loss: 2.1844756603240967: 100%|██████████| 63/63 [00:00<00:00, 101.62it/s]\n",
      "epoch: 80 loss: 2.4041121006011963: 100%|██████████| 63/63 [00:00<00:00, 103.28it/s]\n",
      "epoch: 81 loss: 2.1290483474731445: 100%|██████████| 63/63 [00:00<00:00, 95.75it/s] \n",
      "epoch: 82 loss: 2.079465389251709: 100%|██████████| 63/63 [00:00<00:00, 100.48it/s] \n",
      "epoch: 83 loss: 2.082198143005371: 100%|██████████| 63/63 [00:00<00:00, 101.13it/s] \n",
      "epoch: 84 loss: 2.0614027976989746: 100%|██████████| 63/63 [00:00<00:00, 95.17it/s]\n",
      "epoch: 85 loss: 2.17449688911438: 100%|██████████| 63/63 [00:00<00:00, 98.59it/s]  \n",
      "epoch: 86 loss: 2.176630973815918: 100%|██████████| 63/63 [00:00<00:00, 100.48it/s] \n",
      "epoch: 87 loss: 2.1174228191375732: 100%|██████████| 63/63 [00:00<00:00, 94.60it/s]\n",
      "epoch: 88 loss: 2.101034164428711: 100%|██████████| 63/63 [00:00<00:00, 98.13it/s] \n",
      "epoch: 89 loss: 2.3645596504211426: 100%|██████████| 63/63 [00:00<00:00, 98.44it/s] \n",
      "epoch: 90 loss: 2.308622360229492: 100%|██████████| 63/63 [00:00<00:00, 94.32it/s] \n",
      "epoch: 91 loss: 2.1159980297088623: 100%|██████████| 63/63 [00:00<00:00, 98.91it/s]\n",
      "epoch: 92 loss: 1.9030444622039795: 100%|██████████| 63/63 [00:00<00:00, 101.45it/s]\n",
      "epoch: 93 loss: 1.9155044555664062: 100%|██████████| 63/63 [00:00<00:00, 95.31it/s]\n",
      "epoch: 94 loss: 2.3236751556396484: 100%|██████████| 63/63 [00:00<00:00, 101.29it/s]\n",
      "epoch: 95 loss: 2.0047717094421387: 100%|██████████| 63/63 [00:00<00:00, 99.37it/s] \n",
      "epoch: 96 loss: 1.7666513919830322: 100%|██████████| 63/63 [00:00<00:00, 96.19it/s] \n",
      "epoch: 97 loss: 1.725020408630371: 100%|██████████| 63/63 [00:00<00:00, 96.93it/s] \n",
      "epoch: 98 loss: 1.7671239376068115: 100%|██████████| 63/63 [00:00<00:00, 100.80it/s]\n",
      "epoch: 99 loss: 1.7794198989868164: 100%|██████████| 63/63 [00:00<00:00, 96.78it/s] \n",
      "epoch: 100 loss: 1.570067048072815: 100%|██████████| 63/63 [00:00<00:00, 99.37it/s] \n",
      "epoch: 101 loss: 1.502158522605896: 100%|██████████| 63/63 [00:00<00:00, 100.64it/s] \n",
      "epoch: 102 loss: 1.4211385250091553: 100%|██████████| 63/63 [00:00<00:00, 95.17it/s] \n",
      "epoch: 103 loss: 1.4863817691802979: 100%|██████████| 63/63 [00:00<00:00, 100.48it/s]\n",
      "epoch: 104 loss: 1.5862128734588623: 100%|██████████| 63/63 [00:00<00:00, 102.11it/s]\n",
      "epoch: 105 loss: 1.3774466514587402: 100%|██████████| 63/63 [00:00<00:00, 94.60it/s] \n",
      "epoch: 106 loss: 1.3370182514190674: 100%|██████████| 63/63 [00:00<00:00, 99.53it/s] \n",
      "epoch: 107 loss: 1.3197529315948486: 100%|██████████| 63/63 [00:00<00:00, 102.28it/s]\n",
      "epoch: 108 loss: 1.2978253364562988: 100%|██████████| 63/63 [00:00<00:00, 96.93it/s] \n",
      "epoch: 109 loss: 1.4090852737426758: 100%|██████████| 63/63 [00:00<00:00, 98.75it/s] \n",
      "epoch: 110 loss: 1.2490437030792236: 100%|██████████| 63/63 [00:00<00:00, 101.62it/s]\n",
      "epoch: 111 loss: 1.2131298780441284: 100%|██████████| 63/63 [00:00<00:00, 97.08it/s] \n",
      "epoch: 112 loss: 1.2620437145233154: 100%|██████████| 63/63 [00:00<00:00, 99.69it/s] \n",
      "epoch: 113 loss: 1.1857808828353882: 100%|██████████| 63/63 [00:00<00:00, 99.06it/s] \n",
      "epoch: 114 loss: 1.1484438180923462: 100%|██████████| 63/63 [00:00<00:00, 95.31it/s]\n",
      "epoch: 115 loss: 1.2344330549240112: 100%|██████████| 63/63 [00:00<00:00, 98.75it/s] \n",
      "epoch: 116 loss: 1.1569575071334839: 100%|██████████| 63/63 [00:00<00:00, 100.32it/s]\n",
      "epoch: 117 loss: 1.0930570363998413: 100%|██████████| 63/63 [00:00<00:00, 93.89it/s]\n",
      "epoch: 118 loss: 1.0953490734100342: 100%|██████████| 63/63 [00:00<00:00, 99.06it/s] \n",
      "epoch: 119 loss: 0.9583731889724731: 100%|██████████| 63/63 [00:00<00:00, 99.53it/s] \n",
      "epoch: 120 loss: 0.8393014073371887: 100%|██████████| 63/63 [00:00<00:00, 91.71it/s]\n",
      "epoch: 121 loss: 0.8291742205619812: 100%|██████████| 63/63 [00:00<00:00, 96.78it/s] \n",
      "epoch: 122 loss: 1.0020538568496704: 100%|██████████| 63/63 [00:00<00:00, 100.32it/s]\n",
      "epoch: 123 loss: 1.07772696018219: 100%|██████████| 63/63 [00:00<00:00, 94.74it/s]  \n",
      "epoch: 124 loss: 1.159661889076233: 100%|██████████| 63/63 [00:00<00:00, 100.96it/s] \n",
      "epoch: 125 loss: 0.9238923788070679: 100%|██████████| 63/63 [00:00<00:00, 99.53it/s] \n",
      "epoch: 126 loss: 0.823731541633606: 100%|██████████| 63/63 [00:00<00:00, 93.89it/s] \n",
      "epoch: 127 loss: 0.822610080242157: 100%|██████████| 63/63 [00:00<00:00, 100.97it/s] \n",
      "epoch: 128 loss: 0.964614987373352: 100%|██████████| 63/63 [00:00<00:00, 101.13it/s] \n",
      "epoch: 129 loss: 0.9939748048782349: 100%|██████████| 63/63 [00:00<00:00, 92.92it/s]\n",
      "epoch: 130 loss: 0.801327109336853: 100%|██████████| 63/63 [00:00<00:00, 99.53it/s]  \n",
      "epoch: 131 loss: 0.8099733591079712: 100%|██████████| 63/63 [00:00<00:00, 101.45it/s]\n",
      "epoch: 132 loss: 0.8362890481948853: 100%|██████████| 63/63 [00:00<00:00, 95.03it/s]\n",
      "epoch: 133 loss: 0.6981650590896606: 100%|██████████| 63/63 [00:00<00:00, 98.59it/s]\n",
      "epoch: 134 loss: 0.7197443842887878: 100%|██████████| 63/63 [00:00<00:00, 98.13it/s]\n",
      "epoch: 135 loss: 0.7669373154640198: 100%|██████████| 63/63 [00:00<00:00, 95.46it/s]\n",
      "epoch: 136 loss: 0.8975242376327515: 100%|██████████| 63/63 [00:00<00:00, 96.93it/s]\n",
      "epoch: 137 loss: 0.8811432123184204: 100%|██████████| 63/63 [00:00<00:00, 99.22it/s] \n",
      "epoch: 138 loss: 0.9869066476821899: 100%|██████████| 63/63 [00:00<00:00, 98.90it/s] \n",
      "epoch: 139 loss: 0.9044348001480103: 100%|██████████| 63/63 [00:00<00:00, 96.33it/s]\n",
      "epoch: 140 loss: 0.9701288342475891: 100%|██████████| 63/63 [00:00<00:00, 102.61it/s]\n",
      "epoch: 141 loss: 0.7037197947502136: 100%|██████████| 63/63 [00:00<00:00, 98.29it/s] \n",
      "epoch: 142 loss: 0.6763708591461182: 100%|██████████| 63/63 [00:00<00:00, 96.34it/s]\n",
      "epoch: 143 loss: 0.6645283699035645: 100%|██████████| 63/63 [00:00<00:00, 102.44it/s]\n",
      "epoch: 144 loss: 0.6323637366294861: 100%|██████████| 63/63 [00:00<00:00, 97.83it/s] \n",
      "epoch: 145 loss: 0.6202739477157593: 100%|██████████| 63/63 [00:00<00:00, 97.08it/s]\n",
      "epoch: 146 loss: 0.5090459585189819: 100%|██████████| 63/63 [00:00<00:00, 102.44it/s]\n",
      "epoch: 147 loss: 0.4428110122680664: 100%|██████████| 63/63 [00:00<00:00, 94.46it/s]\n",
      "epoch: 148 loss: 0.40127888321876526: 100%|██████████| 63/63 [00:00<00:00, 97.98it/s]\n",
      "epoch: 149 loss: 0.5034734010696411: 100%|██████████| 63/63 [00:00<00:00, 99.69it/s] \n",
      "epoch: 150 loss: 0.4781680107116699: 100%|██████████| 63/63 [00:00<00:00, 98.60it/s] \n",
      "epoch: 151 loss: 0.454412043094635: 100%|██████████| 63/63 [00:00<00:00, 96.48it/s] \n",
      "epoch: 152 loss: 0.3490679860115051: 100%|██████████| 63/63 [00:00<00:00, 101.78it/s]\n",
      "epoch: 153 loss: 0.5200309157371521: 100%|██████████| 63/63 [00:00<00:00, 95.31it/s]\n",
      "epoch: 154 loss: 0.7264389991760254: 100%|██████████| 63/63 [00:00<00:00, 99.69it/s] \n",
      "epoch: 155 loss: 0.4602511525154114: 100%|██████████| 63/63 [00:00<00:00, 100.64it/s]\n",
      "epoch: 156 loss: 0.3731117248535156: 100%|██████████| 63/63 [00:00<00:00, 96.19it/s] \n",
      "epoch: 157 loss: 0.4207828640937805: 100%|██████████| 63/63 [00:00<00:00, 97.83it/s]\n",
      "epoch: 158 loss: 0.3565763235092163: 100%|██████████| 63/63 [00:00<00:00, 99.22it/s] \n",
      "epoch: 159 loss: 0.33442366123199463: 100%|██████████| 63/63 [00:00<00:00, 93.90it/s]\n",
      "epoch: 160 loss: 0.47963419556617737: 100%|██████████| 63/63 [00:00<00:00, 97.38it/s]\n",
      "epoch: 161 loss: 0.8765656352043152: 100%|██████████| 63/63 [00:00<00:00, 100.00it/s]\n",
      "epoch: 162 loss: 0.5073739886283875: 100%|██████████| 63/63 [00:00<00:00, 97.08it/s] \n",
      "epoch: 163 loss: 0.39210981130599976: 100%|██████████| 63/63 [00:00<00:00, 98.44it/s] \n",
      "epoch: 164 loss: 0.38940930366516113: 100%|██████████| 63/63 [00:00<00:00, 98.75it/s]\n",
      "epoch: 165 loss: 0.24424636363983154: 100%|██████████| 63/63 [00:00<00:00, 97.53it/s]\n",
      "epoch: 166 loss: 0.2763725221157074: 100%|██████████| 63/63 [00:00<00:00, 99.69it/s] \n",
      "epoch: 167 loss: 0.20502670109272003: 100%|██████████| 63/63 [00:00<00:00, 102.44it/s]\n",
      "epoch: 168 loss: 0.2705081105232239: 100%|██████████| 63/63 [00:00<00:00, 97.23it/s] \n",
      "epoch: 169 loss: 0.1680353581905365: 100%|██████████| 63/63 [00:00<00:00, 93.75it/s]\n",
      "epoch: 170 loss: 0.22811689972877502: 100%|██████████| 63/63 [00:00<00:00, 102.95it/s]\n",
      "epoch: 171 loss: 0.18720310926437378: 100%|██████████| 63/63 [00:00<00:00, 98.14it/s]\n",
      "epoch: 172 loss: 0.1686786562204361: 100%|██████████| 63/63 [00:00<00:00, 96.93it/s] \n",
      "epoch: 173 loss: 0.17241394519805908: 100%|██████████| 63/63 [00:00<00:00, 100.32it/s]\n",
      "epoch: 174 loss: 0.1751299500465393: 100%|██████████| 63/63 [00:00<00:00, 97.98it/s] \n",
      "epoch: 175 loss: 0.16387608647346497: 100%|██████████| 63/63 [00:00<00:00, 94.88it/s]\n",
      "epoch: 176 loss: 0.15003877878189087: 100%|██████████| 63/63 [00:00<00:00, 99.69it/s]\n",
      "epoch: 177 loss: 0.20804819464683533: 100%|██████████| 63/63 [00:00<00:00, 95.31it/s] \n",
      "epoch: 178 loss: 0.1479562222957611: 100%|██████████| 63/63 [00:00<00:00, 96.19it/s]\n",
      "epoch: 179 loss: 0.12201008945703506: 100%|██████████| 63/63 [00:00<00:00, 98.59it/s]\n",
      "epoch: 180 loss: 0.1952390968799591: 100%|██████████| 63/63 [00:00<00:00, 97.23it/s] \n",
      "epoch: 181 loss: 0.17620989680290222: 100%|██████████| 63/63 [00:00<00:00, 94.17it/s]\n",
      "epoch: 182 loss: 0.18392428755760193: 100%|██████████| 63/63 [00:00<00:00, 98.13it/s]\n",
      "epoch: 183 loss: 0.13398757576942444: 100%|██████████| 63/63 [00:00<00:00, 97.22it/s]\n",
      "epoch: 184 loss: 0.1788519024848938: 100%|██████████| 63/63 [00:00<00:00, 95.89it/s] \n",
      "epoch: 185 loss: 0.14442577958106995: 100%|██████████| 63/63 [00:00<00:00, 100.32it/s]\n",
      "epoch: 186 loss: 0.19890567660331726: 100%|██████████| 63/63 [00:00<00:00, 96.19it/s]\n",
      "epoch: 187 loss: 0.1141834482550621: 100%|██████████| 63/63 [00:00<00:00, 96.19it/s] \n",
      "epoch: 188 loss: 0.25027772784233093: 100%|██████████| 63/63 [00:00<00:00, 102.44it/s]\n",
      "epoch: 189 loss: 0.34868162870407104: 100%|██████████| 63/63 [00:00<00:00, 99.06it/s] \n",
      "epoch: 190 loss: 0.3982688784599304: 100%|██████████| 63/63 [00:00<00:00, 95.31it/s]\n",
      "epoch: 191 loss: 0.2029929906129837: 100%|██████████| 63/63 [00:00<00:00, 103.28it/s] \n",
      "epoch: 192 loss: 0.14229604601860046: 100%|██████████| 63/63 [00:00<00:00, 100.64it/s]\n",
      "epoch: 193 loss: 0.12415085732936859: 100%|██████████| 63/63 [00:00<00:00, 95.89it/s]\n",
      "epoch: 194 loss: 0.14441969990730286: 100%|██████████| 63/63 [00:00<00:00, 100.80it/s]\n",
      "epoch: 195 loss: 0.08193876594305038: 100%|██████████| 63/63 [00:00<00:00, 100.48it/s]\n",
      "epoch: 196 loss: 0.22415313124656677: 100%|██████████| 63/63 [00:00<00:00, 95.46it/s]\n",
      "epoch: 197 loss: 0.0901486799120903: 100%|██████████| 63/63 [00:00<00:00, 102.44it/s] \n",
      "epoch: 198 loss: 0.21828827261924744: 100%|██████████| 63/63 [00:00<00:00, 98.44it/s] \n",
      "epoch: 199 loss: 0.13478490710258484: 100%|██████████| 63/63 [00:00<00:00, 96.93it/s]\n",
      "epoch: 200 loss: 0.11622388660907745: 100%|██████████| 63/63 [00:00<00:00, 100.16it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim.adam import Adam\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "dataset = TextGeneration()\n",
    "model = LSTM(num_embeddings=len(dataset.BOW)).to(device)\n",
    "loader = DataLoader(dataset, batch_size=64)\n",
    "optim = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(200):\n",
    "    iterator = tqdm.tqdm(loader)\n",
    "    for data, label in iterator:\n",
    "        optim.zero_grad()\n",
    "\n",
    "        pred = model(torch.tensor(data, dtype=torch.long).to(device))\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(pred, torch.tensor(label, dtype=torch.long).to(device))\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        iterator.set_description(f'epoch: {epoch+1} loss: {loss.item()}')\n",
    "\n",
    "torch.save(model.state_dict(), './lstm.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word: i am \n",
      "predicted sentence: i am is pulitzers a thicker community fears even the losers forgot \n"
     ]
    }
   ],
   "source": [
    "def generate(model, BOW, string='i am ', strlen=10):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    print(f'input word: {string}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for p in range(strlen):\n",
    "            words = torch.tensor([BOW[w] for w in string.split()], dtype=torch.long).to(device)\n",
    "\n",
    "            input_tensor = torch.unsqueeze(words[-2:], dim=0)\n",
    "            output = model(input_tensor)\n",
    "            output_word = (torch.argmax(output).cpu().numpy())\n",
    "            string += list(BOW.keys())[output_word]\n",
    "            string += ' '\n",
    "\n",
    "    print(f'predicted sentence: {string}')\n",
    "\n",
    "model.load_state_dict(torch.load('./lstm.pth', map_location=device))\n",
    "pred = generate(model, dataset.BOW)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
